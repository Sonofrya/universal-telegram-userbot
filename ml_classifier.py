"""
–ú–æ–¥—É–ª—å –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Å–æ–æ–±—â–µ–Ω–∏–π
"""
import logging
import numpy as np
from typing import List, Dict, Any, Optional, Tuple
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.model_selection import train_test_split
from sentence_transformers import SentenceTransformer
from database import DatabaseManager
from config import config

class UniversalMessageClassifier:
    """–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä —Å–æ–æ–±—â–µ–Ω–∏–π —Å –∞–≤—Ç–æ–æ–±—É—á–µ–Ω–∏–µ–º"""
    
    def __init__(self, model_name: str = None, db_manager: DatabaseManager = None):
        self.model_name = model_name or config.ml.classifier_model
        self.db_manager = db_manager or DatabaseManager()
        self.classifier = None
        self.sentence_model = None
        self.is_trained = False
        self.training_data = []
        self.last_metrics = {}
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
        self._load_sentence_model()
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –æ–±—É—á–µ–Ω–∏—è
        self._load_training_data()
    
    def _load_sentence_model(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤"""
        try:
            self.sentence_model = SentenceTransformer(config.ml.model_name)
            logging.info(f"‚úÖ –ú–æ–¥–µ–ª—å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {config.ml.model_name}")
        except Exception as e:
            logging.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π: {e}")
            self.sentence_model = None
    
    def _load_training_data(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –æ–±—É—á–µ–Ω–∏—è –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö"""
        try:
            self.training_data = self.db_manager.get_training_data()
            logging.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.training_data)} –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏
            self.last_metrics = self.db_manager.get_latest_metrics(self.model_name) or {}
            
        except Exception as e:
            logging.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö –æ–±—É—á–µ–Ω–∏—è: {e}")
            self.training_data = []
    
    def add_training_example(self, text: str, label: int) -> bool:
        """–î–æ–±–∞–≤–ª—è–µ—Ç –ø—Ä–∏–º–µ—Ä –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
        if not self.sentence_model:
            logging.error("‚ùå –ú–æ–¥–µ–ª—å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
            return False
        
        try:
            # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥
            embedding = self.sentence_model.encode([text])[0]
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö
            success = self.db_manager.save_training_example(text, embedding, label)
            
            if success:
                # –û–±–Ω–æ–≤–ª—è–µ–º –ª–æ–∫–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
                self.training_data.append({
                    'text': text,
                    'embedding': embedding,
                    'label': label
                })
                
                logging.info(f"‚úÖ –î–æ–±–∞–≤–ª–µ–Ω –ø—Ä–∏–º–µ—Ä –æ–±—É—á–µ–Ω–∏—è (–≤—Å–µ–≥–æ: {len(self.training_data)})")
                
                # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ –ø—Ä–∏ –Ω–∞–∫–æ–ø–ª–µ–Ω–∏–∏ –ø—Ä–∏–º–µ—Ä–æ–≤
                if len(self.training_data) >= config.ml.min_training_examples:
                    if len(self.training_data) % config.ml.auto_train_threshold == 0:
                        self.auto_train()
                
                return True
            else:
                return False
                
        except Exception as e:
            logging.error(f"‚ùå –û—à–∏–±–∫–∞ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –ø—Ä–∏–º–µ—Ä–∞ –æ–±—É—á–µ–Ω–∏—è: {e}")
            return False
    
    def auto_train(self) -> bool:
        """–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏"""
        if len(self.training_data) < config.ml.min_training_examples:
            logging.warning(f"‚ùå –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è (–Ω—É–∂–Ω–æ {config.ml.min_training_examples}, –µ—Å—Ç—å {len(self.training_data)})")
            return False
        
        try:
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ
            X = np.array([item['embedding'] for item in self.training_data])
            y = np.array([item['label'] for item in self.training_data])
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤
            unique_labels, counts = np.unique(y, return_counts=True)
            if len(unique_labels) < 2:
                logging.warning("‚ùå –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∫–ª–∞—Å—Å–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
                return False
            
            # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é –º–æ–¥–µ–ª—å –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
            if self.classifier is None:
                self.classifier = LogisticRegression(
                    random_state=42, 
                    max_iter=1000,
                    class_weight='balanced'  # –î–ª—è –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏ –∫–ª–∞—Å—Å–æ–≤
                )
            
            # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å
            self.classifier.fit(X, y)
            self.is_trained = True
            
            # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏
            metrics = self._calculate_metrics(X, y)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö
            self.db_manager.save_model_metrics(self.model_name, metrics)
            self.last_metrics = metrics
            
            logging.info("‚úÖ –ú–æ–¥–µ–ª—å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∞!")
            logging.info(f"üìä –ú–µ—Ç—Ä–∏–∫–∏: –¢–æ—á–Ω–æ—Å—Ç—å: {metrics['accuracy']:.3f}, F1: {metrics['f1']:.3f}")
            
            return True
            
        except Exception as e:
            logging.error(f"‚ùå –û—à–∏–±–∫–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è: {e}")
            return False
    
    def _calculate_metrics(self, X: np.ndarray, y: np.ndarray) -> Dict[str, float]:
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ –º–æ–¥–µ–ª–∏"""
        try:
            y_pred = self.classifier.predict(X)
            
            metrics = {
                'accuracy': accuracy_score(y, y_pred),
                'precision': precision_score(y, y_pred, average='weighted', zero_division=0),
                'recall': recall_score(y, y_pred, average='weighted', zero_division=0),
                'f1': f1_score(y, y_pred, average='weighted', zero_division=0),
                'training_examples': len(self.training_data)
            }
            
            return metrics
            
        except Exception as e:
            logging.error(f"‚ùå –û—à–∏–±–∫–∞ —Ä–∞—Å—á–µ—Ç–∞ –º–µ—Ç—Ä–∏–∫: {e}")
            return {'accuracy': 0.0, 'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'training_examples': 0}
    
    def predict(self, text: str) -> Optional[float]:
        """–ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –¥–ª—è —Ç–µ–∫—Å—Ç–∞"""
        if not self.is_trained or not self.classifier or not self.sentence_model:
            return None
        
        try:
            # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥
            embedding = self.sentence_model.encode([text])[0]
            
            # –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å
            probability = self.classifier.predict_proba([embedding])[0][1]
            return float(probability)
            
        except Exception as e:
            logging.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è: {e}")
            return None
    
    def predict_batch(self, texts: List[str]) -> List[Optional[float]]:
        """–ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –¥–ª—è —Å–ø–∏—Å–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤"""
        if not self.is_trained or not self.classifier or not self.sentence_model:
            return [None] * len(texts)
        
        try:
            # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
            embeddings = self.sentence_model.encode(texts)
            
            # –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏
            probabilities = self.classifier.predict_proba(embeddings)[:, 1]
            return [float(p) for p in probabilities]
            
        except Exception as e:
            logging.error(f"‚ùå –û—à–∏–±–∫–∞ batch –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è: {e}")
            return [None] * len(texts)
    
    def get_stats(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –º–æ–¥–µ–ª–∏"""
        stats = {
            'is_trained': self.is_trained,
            'training_examples': len(self.training_data),
            'model_name': self.model_name,
            'sentence_model_loaded': self.sentence_model is not None
        }
        
        # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å
        if self.last_metrics:
            stats.update({
                'accuracy': self.last_metrics.get('accuracy', 0.0),
                'precision': self.last_metrics.get('precision_score', 0.0),
                'recall': self.last_metrics.get('recall_score', 0.0),
                'f1_score': self.last_metrics.get('f1_score', 0.0),
                'last_training_date': self.last_metrics.get('created_at', '')
            })
        
        return stats
    
    def get_training_data_stats(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –¥–∞–Ω–Ω—ã—Ö –æ–±—É—á–µ–Ω–∏—è"""
        if not self.training_data:
            return {'total': 0, 'positive': 0, 'negative': 0, 'balance': 0.0}
        
        labels = [item['label'] for item in self.training_data]
        positive_count = sum(labels)
        negative_count = len(labels) - positive_count
        
        return {
            'total': len(self.training_data),
            'positive': positive_count,
            'negative': negative_count,
            'balance': positive_count / len(self.training_data) if self.training_data else 0.0
        }
    
    def retrain(self) -> bool:
        """–ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–µ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏"""
        logging.info("üîÑ –ù–∞—á–∏–Ω–∞–µ–º –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–µ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ...")
        return self.auto_train()
    
    def export_model(self, filepath: str) -> bool:
        """–≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ—Ç –º–æ–¥–µ–ª—å –≤ —Ñ–∞–π–ª"""
        try:
            import pickle
            
            model_data = {
                'classifier': self.classifier,
                'model_name': self.model_name,
                'training_data_count': len(self.training_data),
                'metrics': self.last_metrics,
                'exported_at': str(np.datetime64('now'))
            }
            
            with open(filepath, 'wb') as f:
                pickle.dump(model_data, f)
            
            logging.info(f"‚úÖ –ú–æ–¥–µ–ª—å —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–∞ –≤ {filepath}")
            return True
            
        except Exception as e:
            logging.error(f"‚ùå –û—à–∏–±–∫–∞ —ç–∫—Å–ø–æ—Ä—Ç–∞ –º–æ–¥–µ–ª–∏: {e}")
            return False
    
    def import_model(self, filepath: str) -> bool:
        """–ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ—Ç –º–æ–¥–µ–ª—å –∏–∑ —Ñ–∞–π–ª–∞"""
        try:
            import pickle
            
            with open(filepath, 'rb') as f:
                model_data = pickle.load(f)
            
            self.classifier = model_data['classifier']
            self.model_name = model_data.get('model_name', self.model_name)
            self.is_trained = True
            self.last_metrics = model_data.get('metrics', {})
            
            logging.info(f"‚úÖ –ú–æ–¥–µ–ª—å –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–∞ –∏–∑ {filepath}")
            return True
            
        except Exception as e:
            logging.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–º–ø–æ—Ä—Ç–∞ –º–æ–¥–µ–ª–∏: {e}")
            return False
