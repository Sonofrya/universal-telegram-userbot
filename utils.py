"""
–£—Ç–∏–ª–∏—Ç—ã –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å —Ç–µ–∫—Å—Ç–æ–º –∏ –∞–Ω–∞–ª–∏–∑–∞ —Å–æ–æ–±—â–µ–Ω–∏–π
"""
import re
import logging
from typing import List, Optional
from sentence_transformers import SentenceTransformer
from scipy.spatial.distance import cosine
from config import config

def clean_text(text: str) -> str:
    """–û—á–∏—â–∞–µ—Ç —Ç–µ–∫—Å—Ç –æ—Ç –ª–∏—à–Ω–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤"""
    if not text:
        return ""
    
    # –£–¥–∞–ª—è–µ–º —Ö–µ—à—Ç–µ–≥–∏ –∏ —É–ø–æ–º–∏–Ω–∞–Ω–∏—è
    text = re.sub(r'[#@]\w+', '', text)
    
    # –£–¥–∞–ª—è–µ–º URL
    text = re.sub(r'http\S+', '', text)
    
    # –£–¥–∞–ª—è–µ–º –≤—Å–µ –∫—Ä–æ–º–µ –±—É–∫–≤, —Ü–∏—Ñ—Ä –∏ –ø—Ä–æ–±–µ–ª–æ–≤
    text = re.sub(r'[^\w\s–∞-—è–ê-–Ø—ë–Å]', ' ', text)
    
    # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
    text = re.sub(r'\s+', ' ', text).strip()
    
    return text

def contains_full_cycle_phrases(text: str) -> bool:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–∞–ª–∏—á–∏–µ —Ñ—Ä–∞–∑, —É–∫–∞–∑—ã–≤–∞—é—â–∏—Ö –Ω–∞ –ø–æ–ª–Ω—ã–π —Ü–∏–∫–ª"""
    if not text:
        return False
    
    text_lower = text.lower()
    for phrase in config.business.full_cycle_phrases:
        if phrase in text_lower:
            return True
    return False

def is_about_full_cycle_production(text: str) -> bool:
    """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç, –æ—Ç–Ω–æ—Å–∏—Ç—Å—è –ª–∏ —Å–æ–æ–±—â–µ–Ω–∏–µ –∫ –ø–æ–ª–Ω–æ–º—É —Ü–∏–∫–ª—É –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–∞"""
    if not text:
        return False
    
    text_lower = text.lower()
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Ñ—Ä–∞–∑ –æ –ø–æ–ª–Ω–æ–º —Ü–∏–∫–ª–µ
    if contains_full_cycle_phrases(text_lower):
        return True
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤, —É–∫–∞–∑—ã–≤–∞—é—â–∏—Ö –Ω–∞ –ø–æ–ª–Ω—ã–π —Ü–∏–∫–ª
    # –≠—Ç–æ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –ª–æ–≥–∏–∫–∞, –∫–æ—Ç–æ—Ä–∞—è —Ä–∞–±–æ—Ç–∞–µ—Ç –¥–ª—è –ª—é–±–æ–π —Å—Ñ–µ—Ä—ã
    keywords_lower = [kw.lower() for kw in config.business.keywords]
    
    # –ò—â–µ–º —Å–ª–æ–≤–∞, —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ–º/–∫–æ–Ω—Ü–µ–ø—Ü–∏–µ–π
    planning_words = ['–∫–æ–Ω—Ü–µ–ø—Ü', '–∏–¥–µ—è', '–ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω', '—Å—Ç—Ä–∞—Ç–µ–≥', '–∞–Ω–∞–ª–∏–∑', '–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω']
    has_planning = any(word in text_lower for word in planning_words)
    
    # –ò—â–µ–º —Å–ª–æ–≤–∞, —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–æ–º/—Ä–µ–∞–ª–∏–∑–∞—Ü–∏–µ–π
    production_words = ['–ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤', '—Å–æ–∑–¥–∞–Ω', '—Ä–∞–∑—Ä–∞–±–æ—Ç–∫', '—Ä–µ–∞–ª–∏–∑–∞—Ü', '–≤—ã–ø–æ–ª–Ω–µ–Ω']
    has_production = any(word in text_lower for word in production_words)
    
    # –ò—â–µ–º —Å–ª–æ–≤–∞, —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ–º/–ø–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–æ–π
    completion_words = ['–∑–∞–≤–µ—Ä—à–µ–Ω', '–≥–æ—Ç–æ–≤', '—Ñ–∏–Ω–∞–ª—å–Ω', '–∏—Ç–æ–≥–æ–≤', '—Ä–µ–∑—É–ª—å—Ç–∞—Ç']
    has_completion = any(word in text_lower for word in completion_words)
    
    # –ï—Å–ª–∏ –µ—Å—Ç—å –≤—Å–µ —Ç—Ä–∏ —ç—Ç–∞–ø–∞ - —ç—Ç–æ –ø–æ–ª–Ω—ã–π —Ü–∏–∫–ª
    if has_planning and has_production and has_completion:
        return True
    
    # –ï—Å–ª–∏ –µ—Å—Ç—å —É–ø–æ–º–∏–Ω–∞–Ω–∏–µ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —ç—Ç–∞–ø–æ–≤
    stages_count = sum([has_planning, has_production, has_completion])
    if stages_count >= 2 and any(phrase in text_lower for phrase in ['–ø–æ–ª–Ω—ã–π', '–∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π', '–ø–æ–¥ –∫–ª—é—á']):
        return True
    
    return False

def calculate_similarity(model: SentenceTransformer, text: str, keywords: List[str]) -> float:
    """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ —Ç–µ–∫—Å—Ç–∞ —Å –∫–ª—é—á–µ–≤—ã–º–∏ —Å–ª–æ–≤–∞–º–∏"""
    if not text or not keywords:
        return 0.0
    
    try:
        # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
        text_embedding = model.encode([text.lower()])[0]
        keyword_embeddings = model.encode([kw.lower() for kw in keywords])
        
        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º —Å—Ö–æ–¥—Å—Ç–≤–æ
        similarities = [1 - cosine(text_embedding, kw_emb) for kw_emb in keyword_embeddings]
        return max(similarities) if similarities else 0.0
        
    except Exception as e:
        logging.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–∞—Å—á–µ—Ç–µ —Å—Ö–æ–¥—Å—Ç–≤–∞: {e}")
        return 0.0

def contains_blacklisted_words(text: str) -> bool:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–∞–ª–∏—á–∏–µ —Å–ª–æ–≤ –∏–∑ —á–µ—Ä–Ω–æ–≥–æ —Å–ø–∏—Å–∫–∞"""
    if not text:
        return False
    
    text_lower = text.lower()
    for word in config.filter.blacklist_words:
        if re.search(r'\b' + re.escape(word.lower()) + r'\b', text_lower):
            return True
    return False

def is_forward_notification(text: str) -> bool:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ç–µ–∫—Å—Ç —Å–ª—É–∂–µ–±–Ω—ã–º —Å–æ–æ–±—â–µ–Ω–∏–µ–º –æ –ø–µ—Ä–µ—Å—ã–ª–∫–µ"""
    if not text:
        return False
    
    text_lower = text.lower()
    for pattern in config.filter.forward_patterns:
        if re.search(pattern, text_lower, re.IGNORECASE):
            return True
    return False

def is_too_short(text: str) -> bool:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —Å–ª–∏—à–∫–æ–º –ª–∏ –∫–æ—Ä–æ—Ç–∫–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ"""
    if not text:
        return True
    
    words = text.split()
    return len(words) < config.filter.min_message_length

def extract_keywords_from_text(text: str, min_length: int = 3) -> List[str]:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
    if not text:
        return []
    
    # –û—á–∏—â–∞–µ–º —Ç–µ–∫—Å—Ç
    cleaned = clean_text(text)
    
    # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —Å–ª–æ–≤–∞
    words = cleaned.split()
    
    # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ –¥–ª–∏–Ω–µ –∏ —É–±–∏—Ä–∞–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
    stop_words = {'–∏', '–≤', '–Ω–∞', '—Å', '–ø–æ', '–¥–ª—è', '–æ—Ç', '–¥–æ', '–∏–∑', '–∫', '—É', '–æ', '–æ–±', '–∑–∞', '–ø—Ä–∏', '—á–µ—Ä–µ–∑'}
    keywords = [word.lower() for word in words 
                if len(word) >= min_length and word.lower() not in stop_words]
    
    return keywords

def calculate_text_complexity(text: str) -> dict:
    """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç —Å–ª–æ–∂–Ω–æ—Å—Ç—å —Ç–µ–∫—Å—Ç–∞"""
    if not text:
        return {'complexity': 0, 'word_count': 0, 'sentence_count': 0}
    
    words = text.split()
    sentences = re.split(r'[.!?]+', text)
    sentences = [s.strip() for s in sentences if s.strip()]
    
    avg_word_length = sum(len(word) for word in words) / len(words) if words else 0
    avg_sentence_length = len(words) / len(sentences) if sentences else 0
    
    # –ü—Ä–æ—Å—Ç–∞—è –º–µ—Ç—Ä–∏–∫–∞ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏
    complexity = (avg_word_length * 0.3 + avg_sentence_length * 0.7) / 10
    
    return {
        'complexity': complexity,
        'word_count': len(words),
        'sentence_count': len(sentences),
        'avg_word_length': avg_word_length,
        'avg_sentence_length': avg_sentence_length
    }

def format_message_info(message_data: dict) -> str:
    """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å–æ–æ–±—â–µ–Ω–∏–∏ –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏"""
    try:
        info_parts = []
        
        if message_data.get('message_date'):
            info_parts.append(f"üìÖ {message_data['message_date']}")
        
        if message_data.get('sender_info'):
            info_parts.append(f"üë§ {message_data['sender_info']}")
        
        if message_data.get('chat_title'):
            info_parts.append(f"üí¨ {message_data['chat_title']}")
        
        if message_data.get('message_id'):
            info_parts.append(f"üîó ID: {message_data['message_id']}")
        
        if message_data.get('similarity_score') is not None:
            info_parts.append(f"üéØ –°—Ö–æ–¥—Å—Ç–≤–æ: {message_data['similarity_score']:.3f}")
        
        if message_data.get('ml_probability') is not None:
            info_parts.append(f"ü§ñ ML: {message_data['ml_probability']:.3f}")
        
        if message_data.get('is_full_cycle') is not None:
            info_parts.append(f"üîÅ –ü–æ–ª–Ω—ã–π —Ü–∏–∫–ª: {'–î–∞' if message_data['is_full_cycle'] else '–ù–µ—Ç'}")
        
        return '\n'.join(info_parts) + '\n\n'
        
    except Exception as e:
        logging.error(f"‚ùå –û—à–∏–±–∫–∞ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —Å–æ–æ–±—â–µ–Ω–∏–∏: {e}")
        return "‚ùå –û—à–∏–±–∫–∞ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è\n\n"

def validate_config() -> List[str]:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
    errors = []
    
    if not config.telegram.api_id:
        errors.append("TELEGRAM_API_ID –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
    
    if not config.telegram.api_hash:
        errors.append("TELEGRAM_API_HASH –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
    
    if not config.telegram.phone_number:
        errors.append("TELEGRAM_PHONE –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
    
    if not config.business.keywords:
        errors.append("BUSINESS_KEYWORDS –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã")
    
    if not config.business.target_user_ids:
        errors.append("TARGET_USER_IDS –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã")
    
    if config.ml.similarity_threshold < 0 or config.ml.similarity_threshold > 1:
        errors.append("ML_SIMILARITY_THRESHOLD –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –º–µ–∂–¥—É 0 –∏ 1")
    
    if config.filter.min_message_length < 1:
        errors.append("FILTER_MIN_LENGTH –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –±–æ–ª—å—à–µ 0")
    
    return errors

def get_business_domain_examples() -> dict:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—Ä–∏–º–µ—Ä—ã –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Å—Ñ–µ—Ä"""
    return {
        'video_production': {
            'keywords': '–≤–∏–¥–µ–æ–ø—Ä–æ–¥–∞–∫—à–Ω,—Å—ä–µ–º–∫–∞,–º–æ–Ω—Ç–∞–∂,—Ä–µ–∫–ª–∞–º–Ω—ã–µ —Ä–æ–ª–∏–∫–∏,–≤–∏–¥–µ–æ–∫–æ–Ω—Ç–µ–Ω—Ç,—Ü–≤–µ—Ç–æ–∫–æ—Ä—Ä–µ–∫—Ü–∏—è',
            'full_cycle_phrases': '–ø–æ–ª–Ω—ã–π —Ü–∏–∫–ª,–ø–æ–¥ –∫–ª—é—á,–æ—Ç –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏ –¥–æ,—Å—ä–µ–º–∫–∞ –∏ –º–æ–Ω—Ç–∞–∂',
            'description': '–í–∏–¥–µ–æ–ø—Ä–æ–¥–∞–∫—à–Ω –∏ —Å–æ–∑–¥–∞–Ω–∏–µ –≤–∏–¥–µ–æ–∫–æ–Ω—Ç–µ–Ω—Ç–∞'
        },
        'web_development': {
            'keywords': '–≤–µ–±-—Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞,—Å–∞–π—Ç,–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ,frontend,backend,–ø–æ–ª–Ω—ã–π —Ü–∏–∫–ª —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏',
            'full_cycle_phrases': '–ø–æ–ª–Ω—ã–π —Ü–∏–∫–ª —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏,–ø–æ–¥ –∫–ª—é—á,–æ—Ç –¥–∏–∑–∞–π–Ω–∞ –¥–æ,—Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞ –∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ',
            'description': '–í–µ–±-—Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞ –∏ —Å–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π'
        },
        'design': {
            'keywords': '–¥–∏–∑–∞–π–Ω,–ª–æ–≥–æ—Ç–∏–ø,–±—Ä–µ–Ω–¥–∏–Ω–≥,–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–π –¥–∏–∑–∞–π–Ω,UI/UX,–≤–µ–±-–¥–∏–∑–∞–π–Ω',
            'full_cycle_phrases': '–ø–æ–ª–Ω—ã–π —Ü–∏–∫–ª –¥–∏–∑–∞–π–Ω–∞,–ø–æ–¥ –∫–ª—é—á,–æ—Ç –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏ –¥–æ,–¥–∏–∑–∞–π–Ω –∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞',
            'description': '–ì—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–π –¥–∏–∑–∞–π–Ω –∏ –±—Ä–µ–Ω–¥–∏–Ω–≥'
        },
        'marketing': {
            'keywords': '–º–∞—Ä–∫–µ—Ç–∏–Ω–≥,SMM,—Ä–µ–∫–ª–∞–º–∞,–ø—Ä–æ–¥–≤–∏–∂–µ–Ω–∏–µ,–∫–æ–Ω—Ç–µ–Ω—Ç-–º–∞—Ä–∫–µ—Ç–∏–Ω–≥,digital –º–∞—Ä–∫–µ—Ç–∏–Ω–≥',
            'full_cycle_phrases': '–ø–æ–ª–Ω—ã–π —Ü–∏–∫–ª –º–∞—Ä–∫–µ—Ç–∏–Ω–≥–∞,–ø–æ–¥ –∫–ª—é—á,–æ—Ç —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –¥–æ,–ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è',
            'description': '–ú–∞—Ä–∫–µ—Ç–∏–Ω–≥ –∏ –ø—Ä–æ–¥–≤–∏–∂–µ–Ω–∏–µ'
        },
        'photography': {
            'keywords': '—Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏—è,—Ñ–æ—Ç–æ—Å–µ—Å—Å–∏—è,—Å–≤–∞–¥–µ–±–Ω–∞—è —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏—è,–ø–æ—Ä—Ç—Ä–µ—Ç–Ω–∞—è —Å—ä–µ–º–∫–∞,–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–æ—Ç–æ',
            'full_cycle_phrases': '–ø–æ–ª–Ω—ã–π —Ü–∏–∫–ª —Ñ–æ—Ç–æ—Å–µ—Å—Å–∏–∏,–ø–æ–¥ –∫–ª—é—á,–æ—Ç —Å—ä–µ–º–∫–∏ –¥–æ,—Ñ–æ—Ç–æ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞',
            'description': '–§–æ—Ç–æ–≥—Ä–∞—Ñ–∏—è –∏ —Ñ–æ—Ç–æ—Å–µ—Å—Å–∏–∏'
        }
    }

def create_config_template(domain: str) -> str:
    """–°–æ–∑–¥–∞–µ—Ç —à–∞–±–ª–æ–Ω –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –¥–ª—è —É–∫–∞–∑–∞–Ω–Ω–æ–π —Å—Ñ–µ—Ä—ã"""
    examples = get_business_domain_examples()
    
    if domain not in examples:
        domain = 'general'
        template = {
            'keywords': '–∫–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ 1,–∫–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ 2,–∫–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ 3',
            'full_cycle_phrases': '–ø–æ–ª–Ω—ã–π —Ü–∏–∫–ª,–ø–æ–¥ –∫–ª—é—á,–∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π',
            'description': '–û–±—â–∞—è —Å—Ñ–µ—Ä–∞ –¥–µ—è—Ç–µ–ª—å–Ω–æ—Å—Ç–∏'
        }
    else:
        template = examples[domain]
    
    config_template = f"""# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è —Å—Ñ–µ—Ä—ã: {template['description']}
BUSINESS_DOMAIN={domain}
BUSINESS_KEYWORDS={template['keywords']}
FULL_CYCLE_PHRASES={template['full_cycle_phrases']}

# Telegram API –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
TELEGRAM_API_ID=your_api_id_here
TELEGRAM_API_HASH=your_api_hash_here
TELEGRAM_PHONE=your_phone_number

# –¶–µ–ª–µ–≤—ã–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏ (ID —á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é)
TARGET_USER_IDS=user_id_1,user_id_2

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
ML_SIMILARITY_THRESHOLD=0.7
ML_MIN_TRAINING_EXAMPLES=3

# –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è
FILTER_MIN_LENGTH=5
FILTER_BLACKLIST=—Å–ø–∞–º,—Ä–µ–∫–ª–∞–º–∞
"""
    
    return config_template
