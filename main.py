import logging
from telethon import TelegramClient, events
from telethon.sessions import StringSession
from sentence_transformers import SentenceTransformer
from scipy.spatial.distance import cosine
import warnings
import re
import numpy as np
import pickle
import os
from datetime import datetime
from sklearn.linear_model import LogisticRegression

# –û—Ç–∫–ª—é—á–∞–µ–º warnings
warnings.filterwarnings('ignore')

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='[%(levelname)s] %(asctime)s: %(message)s',
    handlers=[logging.FileHandler('userbot.log'), logging.StreamHandler()]
)

# –•—Ä–∞–Ω–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
API_ID = '22794450'
API_HASH = 'eed412e2a2b29fa253407fc60e634e20'
PHONE_NUMBER = '79538703044'

# –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ —Ü–∏–∫–ª–∞ –ø—Ä–æ–¥–∞–∫—à–µ–Ω–∞
KEYWORDS_STRING = (
    '–ø–æ–ª–Ω—ã–π —Ü–∏–∫–ª, –≤–∏–¥–µ–æ–ø—Ä–æ–¥—é—Å–µ—Ä—Å–∫–∞—è –∫–æ–º–ø–∞–Ω–∏—è, –ø—Ä–æ–¥–∞–∫—à–Ω –ø–æ–ª–Ω–æ–≥–æ —Ü–∏–∫–ª–∞, '
    '–∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –ø—Ä–æ–¥–∞–∫—à–Ω, –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–æ –≤–∏–¥–µ–æ –ø–æ–¥ –∫–ª—é—á, —Å—ä–µ–º–∫–∞ –ø–æ–¥ –∫–ª—é—á, '
    '—Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏, –Ω–∞–ø–∏—Å–∞–Ω–∏–µ —Å—Ü–µ–Ω–∞—Ä–∏—è, —Å—Ü–µ–Ω–∞—Ä–∏–π, —Å—ä–µ–º–∫–∞, –º–æ–Ω—Ç–∞–∂, '
    '—Ü–≤–µ—Ç–æ–∫–æ—Ä—Ä–µ–∫—Ü–∏—è, —É–ø–∞–∫–æ–≤–∫–∞ –≤–∏–¥–µ–æ, —Ä–µ–∫–ª–∞–º–Ω—ã–µ —Ä–∏–ª—Å—ã, —Ä–µ–∫–ª–∞–º–Ω—ã–µ reels, '
    '—Ä–µ–∫–ª–∞–º–Ω—ã–µ —Ä–æ–ª–∏–∫–∏, –≤–∏–¥–µ–æ–ø—Ä–æ–¥–∞–∫—à–Ω, –≤–∏–¥–µ–æ–ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–æ, –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–æ –≤–∏–¥–µ–æ, '
    '–≤–∏–¥–µ–æ–∫–æ–Ω—Ç–µ–Ω—Ç, –≤–∏–¥–µ–æ—Ä–æ–ª–∏–∫–∏, –≤–∏–¥–µ–æ –¥–ª—è –±—Ä–µ–Ω–¥–∞, –≤–∏–¥–µ–æ –¥–ª—è –±–∏–∑–Ω–µ—Å–∞, —Ä–∏–ª—Å, reels, —Å–Ω–∏–ø–ø–µ—Ç'
)
KEYWORDS = [kw.strip().lower() for kw in KEYWORDS_STRING.split(', ')]
TARGET_USER_IDS = ['7013831967', '406521857']
SIMILARITY_THRESHOLD = 0.7
SESSION_FILE = 'session.txt'

# –ß–µ—Ä–Ω—ã–π —Å–ø–∏—Å–æ–∫
BLACKLIST_WORDS = [
    '—Ñ—Ä–∏–ª–∞–Ω—Å', '—Ñ—Ä–∏–ª–∞–Ω—Å–µ—Ä'
]

# –§—Ä–∞–∑—ã –ø–æ–ª–Ω–æ–≥–æ —Ü–∏–∫–ª–∞
FULL_CYCLE_PHRASES = [
    '–ø–æ–ª–Ω—ã–π —Ü–∏–∫–ª', '–ø–æ–¥ –∫–ª—é—á', '–ø—Ä–æ–¥–∞–∫—à–Ω –ø–æ–ª–Ω–æ–≥–æ —Ü–∏–∫–ª–∞', '–∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π', '–æ—Ç –∏ –¥–æ', 
    '–æ—Ç –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏ –¥–æ', '–æ—Ç —Å—ä–µ–º–∫–∏ –¥–æ', '–æ—Ç –∏–¥–µ–∏ –¥–æ',
    '—Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏', '–Ω–∞–ø–∏—Å–∞–Ω–∏–µ —Å—Ü–µ–Ω–∞—Ä–∏—è', '—Å—ä–µ–º–∫–∞ –∏ –º–æ–Ω—Ç–∞–∂',
    '–º–æ–Ω—Ç–∞–∂ –∏ —Ü–≤–µ—Ç–æ–∫–æ—Ä—Ä–µ–∫—Ü–∏—è', '—Ü–≤–µ—Ç–æ–∫–æ—Ä—Ä–µ–∫—Ü–∏—è –∏ —É–ø–∞–∫–æ–≤–∫–∞',
    '–≤–µ—Å—å —Ü–∏–∫–ª –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–∞', '–ø–æ–ª–Ω–æ–µ –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–æ'
]

FORWARD_PATTERNS = [
    r'–ø–µ—Ä–µ—Å–ª–∞–Ω–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ', r'forwarded message', r'–±—ã–ª–æ –ø–µ—Ä–µ—Å–ª–∞–Ω–æ',
    r'forwarded from', r'—Å–æ–æ–±—â–µ–Ω–∏–µ –ø–µ—Ä–µ—Å–ª–∞–Ω–æ', r'–º–µ–¥–∏–∞ –ø–µ—Ä–µ—Å–ª–∞–Ω–æ'
]

processed_messages = set()
feedback_db = {}

def clean_text(text):
    """–û—á–∏—â–∞–µ—Ç —Ç–µ–∫—Å—Ç –æ—Ç –ª–∏—à–Ω–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤"""
    if not text:
        return ""
    
    text = re.sub(r'[#@]\w+', '', text)
    text = re.sub(r'http\S+', '', text)
    text = re.sub(r'[^\w\s–∞-—è–ê-–Ø—ë–Å]', ' ', text)
    text = re.sub(r'\s+', ' ', text).strip()
    
    return text

def contains_full_cycle_phrases(text):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–∞–ª–∏—á–∏–µ —Ñ—Ä–∞–∑, —É–∫–∞–∑—ã–≤–∞—é—â–∏—Ö –Ω–∞ –ø–æ–ª–Ω—ã–π —Ü–∏–∫–ª"""
    if not text:
        return False
    
    text_lower = text.lower()
    for phrase in FULL_CYCLE_PHRASES:
        if phrase in text_lower:
            return True
    return False

def is_about_full_cycle_production(text):
    """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç, –æ—Ç–Ω–æ—Å–∏—Ç—Å—è –ª–∏ —Å–æ–æ–±—â–µ–Ω–∏–µ –∫ –ø–æ–ª–Ω–æ–º—É —Ü–∏–∫–ª—É –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–∞"""
    if not text:
        return False
    
    text_lower = text.lower()
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Ñ—Ä–∞–∑ –æ –ø–æ–ª–Ω–æ–º —Ü–∏–∫–ª–µ
    if contains_full_cycle_phrases(text_lower):
        return True
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤, —É–∫–∞–∑—ã–≤–∞—é—â–∏—Ö –Ω–∞ –ø–æ–ª–Ω—ã–π —Ü–∏–∫–ª
    has_concept = any(word in text_lower for word in ['–∫–æ–Ω—Ü–µ–ø—Ü', '–∏–¥–µ—è', '—Å—Ü–µ–Ω–∞—Ä–∏–π'])
    has_production = any(word in text_lower for word in ['—Å—ä–µ–º–∫', '–ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤', '–º–æ–Ω—Ç–∞–∂'])
    has_post = any(word in text_lower for word in ['—Ü–≤–µ—Ç–æ–∫–æ—Ä—Ä–µ–∫—Ü', '—É–ø–∞–∫–æ–≤–∫', '–≥—Ä–∞—Ñ–∏–∫'])
    
    # –ï—Å–ª–∏ –µ—Å—Ç—å –≤—Å–µ —Ç—Ä–∏ —ç—Ç–∞–ø–∞ - —ç—Ç–æ –ø–æ–ª–Ω—ã–π —Ü–∏–∫–ª
    if has_concept and has_production and has_post:
        return True
    
    # –ï—Å–ª–∏ –µ—Å—Ç—å —É–ø–æ–º–∏–Ω–∞–Ω–∏–µ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —ç—Ç–∞–ø–æ–≤
    stages_count = sum([has_concept, has_production, has_post])
    if stages_count >= 2 and '–ø–æ–ª–Ω—ã–π' in text_lower:
        return True
    
    return False

def calculate_similarity(model, text, keyword_embeddings):
    """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ —Ç–µ–∫—Å—Ç–∞ —Å –∫–ª—é—á–µ–≤—ã–º–∏ —Å–ª–æ–≤–∞–º–∏"""
    if not text:
        return 0.0
    
    try:
        text_embedding = model.encode([text.lower()])[0]
        similarities = [1 - cosine(text_embedding, kw_emb) for kw_emb in keyword_embeddings]
        return max(similarities) if similarities else 0.0
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–∞—Å—á–µ—Ç–µ —Å—Ö–æ–¥—Å—Ç–≤–∞: {e}")
        return 0.0

def contains_blacklisted_words(text):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–∞–ª–∏—á–∏–µ —Å–ª–æ–≤ –∏–∑ —á–µ—Ä–Ω–æ–≥–æ —Å–ø–∏—Å–∫–∞"""
    if not text:
        return False
    
    text_lower = text.lower()
    for word in BLACKLIST_WORDS:
        if re.search(r'\b' + re.escape(word) + r'\b', text_lower):
            return True
    return False

def is_forward_notification(text):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ç–µ–∫—Å—Ç —Å–ª—É–∂–µ–±–Ω—ã–º —Å–æ–æ–±—â–µ–Ω–∏–µ–º –æ –ø–µ—Ä–µ—Å—ã–ª–∫–µ"""
    if not text:
        return False
    
    text_lower = text.lower()
    for pattern in FORWARD_PATTERNS:
        if re.search(pattern, text_lower, re.IGNORECASE):
            return True
    return False

def is_too_short(text):
    """–ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è"""
    if not text:
        return True
    words = text.split()
    return len(words) < 5

def get_sender_info(sender):
    """–ü–æ–ª—É—á–∞–µ—Ç –ø–æ–ª–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –æ—Ç–ø—Ä–∞–≤–∏—Ç–µ–ª–µ"""
    if not sender:
        return "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –æ—Ç–ø—Ä–∞–≤–∏—Ç–µ–ª—å"
    
    info_parts = []
    
    if hasattr(sender, 'first_name') and sender.first_name:
        info_parts.append(sender.first_name)
    if hasattr(sender, 'last_name') and sender.last_name:
        info_parts.append(sender.last_name)
    if hasattr(sender, 'username') and sender.username:
        info_parts.append(f"(@{sender.username})")
    if hasattr(sender, 'title') and sender.title:
        info_parts.append(sender.title)
    
    return ' '.join(info_parts) if info_parts else "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –æ—Ç–ø—Ä–∞–≤–∏—Ç–µ–ª—å"

async def get_entity(client, user_id):
    try:
        # –ü—Ä–æ–±—É–µ–º –∫–∞–∫ —á–∏—Å–ª–æ (–µ—Å–ª–∏ —ç—Ç–æ —Ü–∏—Ñ—Ä–æ–≤–æ–π ID)
        if isinstance(user_id, str) and user_id.isdigit():
            user_entity = await client.get_entity(int(user_id))
        else:
            # –ü—Ä–æ–±—É–µ–º –∫–∞–∫ username/–Ω–æ–º–µ—Ä —Ç–µ–ª–µ—Ñ–æ–Ω–∞
            user_entity = await client.get_entity(str(user_id))
        return user_entity
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Å—É—â–Ω–æ—Å—Ç–∏ –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {user_id}: {e}")
        return None

async def copy_message_content(client, message, target_user):
    """–ö–æ–ø–∏—Ä—É–µ—Ç —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Å–æ–æ–±—â–µ–Ω–∏—è –≤–º–µ—Å—Ç–æ –ø–µ—Ä–µ—Å—ã–ª–∫–∏."""
    try:
        sent_message = None
        
        if message.text:
            sent_message = await client.send_message(target_user, message.text)
        
        if message.media and not isinstance(message.media, type(None)):
            if sent_message:
                await client.send_file(target_user, message.media, reply_to=sent_message.id)
            else:
                sent_message = await client.send_file(target_user, message.media)
        
        return sent_message
        
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–∏ —Å–æ–æ–±—â–µ–Ω–∏—è: {e}")
        return None

class MessageClassifier:
    def __init__(self, model_name='production_classifier'):
        self.model_name = model_name
        self.classifier = None
        self.training_data = []
        self.is_trained = False
        self.last_training_size = 0
        self.load_model()
    
    def load_model(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –∏ –¥–∞–Ω–Ω—ã–µ –æ–±—É—á–µ–Ω–∏—è —Å –¥–∏—Å–∫–∞"""
        try:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
            if os.path.exists(f'{self.model_name}.pkl'):
                with open(f'{self.model_name}.pkl', 'rb') as f:
                    data = pickle.load(f)
                    self.classifier = data['classifier']
                    self.is_trained = True
                    logging.info("‚úì –ú–æ–¥–µ–ª—å –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –æ–±—É—á–µ–Ω–∏—è
            if os.path.exists(f'{self.model_name}_data.pkl'):
                with open(f'{self.model_name}_data.pkl', 'rb') as f:
                    self.training_data = pickle.load(f)
                    self.last_training_size = len(self.training_data)
                    logging.info(f"‚úì –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.training_data)} –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
                    
        except Exception as e:
            logging.error(f"‚úó –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
            self.classifier = LogisticRegression(random_state=42, max_iter=1000)
            self.training_data = []
            self.is_trained = False
    
    def save_model(self):
        """–°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å –∏ –¥–∞–Ω–Ω—ã–µ –æ–±—É—á–µ–Ω–∏—è –Ω–∞ –¥–∏—Å–∫"""
        try:
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å
            if self.classifier and self.is_trained:
                with open(f'{self.model_name}.pkl', 'wb') as f:
                    pickle.dump({
                        'classifier': self.classifier,
                        'saved_at': datetime.now(),
                        'training_size': len(self.training_data)
                    }, f)
            
            # –í—Å–µ–≥–¥–∞ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ –æ–±—É—á–µ–Ω–∏—è
            with open(f'{self.model_name}_data.pkl', 'wb') as f:
                pickle.dump(self.training_data, f)
                
            logging.info("‚úì –ú–æ–¥–µ–ª—å –∏ –¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –Ω–∞ –¥–∏—Å–∫")
            
        except Exception as e:
            logging.error(f"‚úó –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–∏: {e}")
    
    def add_training_example(self, text, label, model):
        """–î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–∏–º–µ—Ä –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–±—É—á–∞–µ–º –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏"""
        try:
            # –û—á–∏—â–∞–µ–º —Ç–µ–∫—Å—Ç –∏ –ø–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥
            cleaned_text = clean_text(text)
            embedding = model.encode([cleaned_text])[0]
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ
            self.training_data.append({
                'text': text,
                'embedding': embedding,
                'label': label,
                'added_at': datetime.now()
            })
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–∞ –¥–∏—Å–∫
            self.save_model()
            
            logging.info(f"‚úì –î–æ–±–∞–≤–ª–µ–Ω –ø—Ä–∏–º–µ—Ä –æ–±—É—á–µ–Ω–∏—è (–≤—Å–µ–≥–æ: {len(self.training_data)})")
            
            # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ –ø—Ä–∏ –Ω–∞–∫–æ–ø–ª–µ–Ω–∏–∏ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –ø—Ä–∏–º–µ—Ä–æ–≤
            if len(self.training_data) >= 3 and len(self.training_data) % 2 == 0:
                self.auto_train(model)
            
        except Exception as e:
            logging.error(f"‚úó –û—à–∏–±–∫–∞ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –ø—Ä–∏–º–µ—Ä–∞: {e}")
    
    def auto_train(self, model):
        """–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ –ø—Ä–∏ –Ω–∞–∫–æ–ø–ª–µ–Ω–∏–∏ –ø—Ä–∏–º–µ—Ä–æ–≤"""
        if len(self.training_data) < 3:
            return False
        
        try:
            # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é –º–æ–¥–µ–ª—å –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
            if self.classifier is None:
                self.classifier = LogisticRegression(random_state=42, max_iter=1000)
            
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ
            X = np.array([item['embedding'] for item in self.training_data])
            y = np.array([item['label'] for item in self.training_data])
            
            # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å
            self.classifier.fit(X, y)
            self.is_trained = True
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å
            self.save_model()
            
            accuracy = self.classifier.score(X, y)
            self.last_training_size = len(self.training_data)
            
            logging.info(f"‚úÖ –ú–æ–¥–µ–ª—å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∞!")
            logging.info(f"üìä –ù–∞ {len(self.training_data)} –ø—Ä–∏–º–µ—Ä–∞—Ö, —Ç–æ—á–Ω–æ—Å—Ç—å: {accuracy:.2f}")
            
            return True
            
        except Exception as e:
            logging.error(f"‚úó –û—à–∏–±–∫–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è: {e}")
            return False
    
    def train(self, model):
        """–†—É—á–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏"""
        return self.auto_train(model)
    
    def predict(self, text, model):
        """–ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –¥–ª—è —Ç–µ–∫—Å—Ç–∞"""
        if not self.is_trained or self.classifier is None:
            return None
        
        try:
            cleaned_text = clean_text(text)
            embedding = model.encode([cleaned_text])[0]
            probability = self.classifier.predict_proba([embedding])[0][1]
            return probability
            
        except Exception as e:
            logging.error(f"‚úó –û—à–∏–±–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è: {e}")
            return None
    
    def get_stats(self):
        """–ü–æ–ª—É—á–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –º–æ–¥–µ–ª–∏"""
        if not self.is_trained or len(self.training_data) == 0:
            return {
                'is_trained': False,
                'training_examples': len(self.training_data),
                'accuracy': None
            }
        
        try:
            X = np.array([item['embedding'] for item in self.training_data])
            y = np.array([item['label'] for item in self.training_data])
            accuracy = self.classifier.score(X, y)
            
            return {
                'is_trained': True,
                'training_examples': len(self.training_data),
                'accuracy': accuracy,
                'last_training_size': self.last_training_size
            }
        except:
            return {
                'is_trained': False,
                'training_examples': len(self.training_data),
                'accuracy': None
            }

async def forward_messages(event, model, keyword_embeddings, client, classifier):
    """–ü–µ—Ä–µ—Å—ã–ª–∫–∞ —Å–æ–æ–±—â–µ–Ω–∏–π —Ç–æ–ª—å–∫–æ –æ –ø–æ–ª–Ω–æ–º —Ü–∏–∫–ª–µ –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–∞"""
    
    # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–≤–æ–∏ —Å–æ–æ–±—â–µ–Ω–∏—è –∏ —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ
    if event.message.out or event.message.id in processed_messages:
        return
    
    message_text = event.message.text if event.message.text else ""
    cleaned_text = clean_text(message_text)
    
    # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è –æ –ø–µ—Ä–µ—Å—ã–ª–∫–µ
    if is_forward_notification(message_text):
        logging.info("–ü—Ä–æ–ø—É—â–µ–Ω–æ —Å–ª—É–∂–µ–±–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –æ –ø–µ—Ä–µ—Å—ã–ª–∫–µ")
        return
    
    # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è
    if is_too_short(cleaned_text):
        logging.info(f"–ü—Ä–æ–ø—É—â–µ–Ω–æ –∫–æ—Ä–æ—Ç–∫–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ: '{cleaned_text}'")
        return
    
    # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è –∏–∑ —á–µ—Ä–Ω–æ–≥–æ —Å–ø–∏—Å–∫–∞
    if contains_blacklisted_words(message_text):
        logging.info(f"–ü—Ä–æ–ø—É—â–µ–Ω–æ —Å–æ–æ–±—â–µ–Ω–∏–µ –∏–∑ —á–µ—Ä–Ω–æ–≥–æ —Å–ø–∏—Å–∫–∞: '{cleaned_text}'")
        return
    
    processed_messages.add(event.message.id)

    logging.info(f"‚úó –°–æ–æ–±—â–µ–Ω–∏–µ –Ω–µ –ø–µ—Ä–µ—Å–ª–∞–Ω–æ [ID: {event.message.id}]")
    
    try:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –æ—Ç–Ω–æ—Å–∏—Ç—Å—è –ª–∏ —Å–æ–æ–±—â–µ–Ω–∏–µ –∫ –ø–æ–ª–Ω–æ–º—É —Ü–∏–∫–ª—É –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–∞
        is_full_cycle = is_about_full_cycle_production(message_text)
        
        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ
        max_similarity = calculate_similarity(model, cleaned_text, keyword_embeddings)
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º ML –º–æ–¥–µ–ª—å –µ—Å–ª–∏ –æ–Ω–∞ –æ–±—É—á–µ–Ω–∞
        ml_probability = classifier.predict(message_text, model)
        use_ml = ml_probability is not None and classifier.is_trained
        
        # –†–µ—à–∞–µ–º, –ø–µ—Ä–µ—Å—ã–ª–∞—Ç—å –ª–∏ —Å–æ–æ–±—â–µ–Ω–∏–µ
        if use_ml:
            should_forward = ml_probability > 0.5
            logging.info(f"ML –º–æ–¥–µ–ª—å: {ml_probability:.3f}, –†–µ—à–µ–Ω–∏–µ: {'‚úì' if should_forward else '‚úó'}")
        else:
            should_forward = is_full_cycle or max_similarity > SIMILARITY_THRESHOLD
        
        logging.info(f"–°–æ–æ–±—â–µ–Ω–∏–µ: '{cleaned_text[:80]}...'")
        logging.info(f"–°—Ö–æ–¥—Å—Ç–≤–æ: {max_similarity:.3f}, –ü–æ–ª–Ω—ã–π —Ü–∏–∫–ª: {is_full_cycle}, ML: {ml_probability}")
        
        if should_forward:
            chat_title = event.message.chat.title if hasattr(event.message.chat, 'title') else "–ü—Ä–∏–≤–∞—Ç–Ω—ã–π —á–∞—Ç"
            
            sender_info = "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –æ—Ç–ø—Ä–∞–≤–∏—Ç–µ–ª—å"
            try:
                sender = await event.message.get_sender()
                sender_info = get_sender_info(sender)
            except Exception as e:
                logging.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –æ—Ç–ø—Ä–∞–≤–∏—Ç–µ–ª–µ: {e}")
            
            message_date = event.message.date.strftime("%d.%m.%Y %H:%M") if hasattr(event.message, 'date') else ""
            ml_info = f", ML: {ml_probability:.3f}" if use_ml else ""
            
            message_info = (
                f"üìÖ {message_date}\nüë§ {sender_info}\nüí¨ {chat_title}\n"
                f"üîó ID: {event.message.id}\nüéØ –°—Ö–æ–¥—Å—Ç–≤–æ: {max_similarity:.3f}{ml_info}\n"
                f"üîÅ –ü–æ–ª–Ω—ã–π —Ü–∏–∫–ª: {'–î–∞' if is_full_cycle else '–ù–µ—Ç'}\n\n"
            )
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏
            feedback_db[event.message.id] = {
                'text': message_text, 
                'forwarded': True, 
                'timestamp': datetime.now(),
                'similarity': max_similarity,
                'is_full_cycle': is_full_cycle,
                'ml_probability': ml_probability
            }
            
            for user_id in TARGET_USER_IDS:
                try:
                    user_entity = await get_entity(client, user_id)
                    if not user_entity:
                        continue
                    
                    try:
                        forward_message = await client.forward_messages(user_entity, event.message)
                        if forward_message:
                            await client.send_message(user_entity, message_info, reply_to=forward_message.id)
                            logging.info(f"‚úì –°–æ–æ–±—â–µ–Ω–∏–µ –ø–µ—Ä–µ—Å–ª–∞–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é {user_id}")
                            continue
                    except Exception as forward_error:
                        logging.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–µ—Ä–µ—Å–ª–∞—Ç—å: {forward_error}")
                    
                    copied_message = await copy_message_content(client, event.message, user_entity)
                    if copied_message:
                        await client.send_message(user_entity, message_info, reply_to=copied_message.id)
                        logging.info(f"‚úì –°–æ–¥–µ—Ä–∂–∏–º–æ–µ —Å–∫–æ–ø–∏—Ä–æ–≤–∞–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é {user_id}")
                    else:
                        logging.error(f"‚úó –ù–µ —É–¥–∞–ª–æ—Å—å —Å–∫–æ–ø–∏—Ä–æ–≤–∞—Ç—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é {user_id}")
                        
                except Exception as e:
                    logging.error(f"‚úó –û—à–∏–±–∫–∞ –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {user_id}: {e}")
        else:
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–π –ø—Ä–∏–º–µ—Ä –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
            feedback_db[event.message.id] = {
                'text': message_text, 
                'forwarded': False, 
                'timestamp': datetime.now(),
                'similarity': max_similarity,
                'is_full_cycle': is_full_cycle,
                'ml_probability': ml_probability
            }
            logging.info(f"‚úó –°–æ–æ–±—â–µ–Ω–∏–µ –Ω–µ –ø–µ—Ä–µ—Å–ª–∞–Ω–æ [ID: {event.message.id}]")
                    
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ —Å–æ–æ–±—â–µ–Ω–∏—è: {e}")

async def main():
    # –ó–∞–≥—Ä—É–∑–∫–∞ —Å–µ—Å—Å–∏–∏ –µ—Å–ª–∏ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
    session_string = None
    if os.path.exists(SESSION_FILE):
        with open(SESSION_FILE, 'r') as f:
            session_string = f.read().strip()
    
    if session_string:
        client = TelegramClient(StringSession(session_string), API_ID, API_HASH)
    else:
        client = TelegramClient(StringSession(), API_ID, API_HASH)
    
    with warnings.catch_warnings():
        warnings.simplefilter("ignore")
        model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
        keyword_embeddings = model.encode(KEYWORDS)
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä —Å –∞–≤—Ç–æ–æ–±—É—á–µ–Ω–∏–µ–º
    classifier = MessageClassifier()
    stats = classifier.get_stats()
    logging.info(f"–ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {'–æ–±—É—á–µ–Ω–∞' if stats['is_trained'] else '–Ω–µ –æ–±—É—á–µ–Ω–∞'}")
    logging.info(f"–ü—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è: {stats['training_examples']}")
    if stats['accuracy']:
        logging.info(f"–¢–æ—á–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏: {stats['accuracy']:.2%}")
    
    # –ö–æ–º–∞–Ω–¥—ã –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è
    @client.on(events.NewMessage(pattern='/train'))
    async def train_handler(event):
        if len(classifier.training_data) >= 3:
            success = classifier.train(model)
            await event.reply("‚úÖ –ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∞!" if success else "‚ùå –û—à–∏–±–∫–∞ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è")
        else:
            await event.reply(f"‚ùå –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö. –ù—É–∂–Ω–æ 3+ –ø—Ä–∏–º–µ—Ä–æ–≤ (—Å–µ–π—á–∞—Å: {len(classifier.training_data)})")
    
    @client.on(events.NewMessage(pattern='/stats'))
    async def stats_handler(event):
        stats = classifier.get_stats()
        accuracy_text = f"{stats['accuracy']:.2%}" if stats['accuracy'] else "N/A"
        stats_text = (
            f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –º–æ–¥–µ–ª–∏:\n"
            f"‚Ä¢ –û–±—É—á–µ–Ω–∞: {'‚úÖ' if stats['is_trained'] else '‚ùå'}\n"
            f"‚Ä¢ –ü—Ä–∏–º–µ—Ä–æ–≤: {stats['training_examples']}\n"
            f"‚Ä¢ –¢–æ—á–Ω–æ—Å—Ç—å: {accuracy_text}\n"
            f"‚Ä¢ –°–æ–æ–±—â–µ–Ω–∏–π –≤ –∏—Å—Ç–æ—Ä–∏–∏: {len(feedback_db)}"
        )
        await event.reply(stats_text)
    
    @client.on(events.NewMessage(pattern=r'/correct_(\d+)'))
    async def correct_handler(event):
        try:
            msg_id = int(event.pattern_match.group(1))
            if msg_id in feedback_db:
                classifier.add_training_example(feedback_db[msg_id]['text'], 1, model)
                await event.reply("‚úÖ –î–æ–±–∞–≤–ª–µ–Ω –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–π –ø—Ä–∏–º–µ—Ä –æ–±—É—á–µ–Ω–∏—è!")
            else:
                await event.reply("‚ùå –°–æ–æ–±—â–µ–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –≤ –∏—Å—Ç–æ—Ä–∏–∏")
        except:
            await event.reply("‚ùå –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ: /correct_12345")
    
    @client.on(events.NewMessage(pattern=r'/wrong_(\d+)'))
    async def wrong_handler(event):
        try:
            msg_id = int(event.pattern_match.group(1))
            if msg_id in feedback_db:
                classifier.add_training_example(feedback_db[msg_id]['text'], 0, model)
                await event.reply("‚úÖ –î–æ–±–∞–≤–ª–µ–Ω –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–π –ø—Ä–∏–º–µ—Ä –æ–±—É—á–µ–Ω–∏—è!")
            else:
                await event.reply("‚ùå –°–æ–æ–±—â–µ–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –≤ –∏—Å—Ç–æ—Ä–∏–∏")
        except:
            await event.reply("‚ùå –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ: /wrong_12345")
    
    @client.on(events.NewMessage(pattern='/clear_history'))
    async def clear_history_handler(event):
        feedback_db.clear()
        await event.reply("‚úÖ –ò—Å—Ç–æ—Ä–∏—è —Å–æ–æ–±—â–µ–Ω–∏–π –æ—á–∏—â–µ–Ω–∞!")
    
    # –û—Å–Ω–æ–≤–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç—á–∏–∫ —Å–æ–æ–±—â–µ–Ω–∏–π
    @client.on(events.NewMessage)
    async def handler(event):
        await forward_messages(event, model, keyword_embeddings, client, classifier)

    logging.info("–Æ–∑–µ—Ä–±–æ—Ç –∑–∞–ø—É—â–µ–Ω —Å –∞–≤—Ç–æ–æ–±—É—á–µ–Ω–∏–µ–º!")
    logging.info("–î–æ—Å—Ç—É–ø–Ω—ã–µ –∫–æ–º–∞–Ω–¥—ã: /train, /stats, /correct_12345, /wrong_12345, /clear_history")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–µ—Å—Å–∏—é –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –∑–∞–ø—É—Å–∫–µ
    if not os.path.exists(SESSION_FILE):
        await client.start(phone=PHONE_NUMBER)
        with open(SESSION_FILE, 'w') as f:
            f.write(client.session.save())
        logging.info("–°–µ—Å—Å–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ —Ñ–∞–π–ª")
    else:
        await client.start()
    
    # üîß –î–û–ë–ê–í–õ–ï–ù–û: –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ —Å—É—â–Ω–æ—Å—Ç–µ–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
    try:
        logging.info("üîç –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ —Å—É—â–Ω–æ—Å—Ç–µ–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π...")
        for user_id in TARGET_USER_IDS:
            try:
                if str(user_id).isdigit():
                    await client.get_entity(int(user_id))
                    logging.info(f"‚úÖ –°—É—â–Ω–æ—Å—Ç—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {user_id} –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
                else:
                    await client.get_entity(str(user_id))
                    logging.info(f"‚úÖ –°—É—â–Ω–æ—Å—Ç—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {user_id} –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
            except Exception as e:
                logging.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—É—â–Ω–æ—Å—Ç—å –¥–ª—è {user_id}: {e}")
                logging.info("‚ÑπÔ∏è –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –æ—Ç–ø—Ä–∞–≤–∏—Ç—å —Å–æ–æ–±—â–µ–Ω–∏–µ —ç—Ç–æ–º—É –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é –æ—Ç –∏–º–µ–Ω–∏ –±–æ—Ç–∞")
    except Exception as e:
        logging.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–µ —Å—É—â–Ω–æ—Å—Ç–µ–π: {e}")
    
    await client.run_until_disconnected()

if __name__ == "__main__":
    import asyncio
    asyncio.run(main())